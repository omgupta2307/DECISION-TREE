{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ed96a-5ba0-47e8-b56c-1df9d9e4ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                           Decision Tree\n",
    "                                                  ________________________________\n",
    "Q1.Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
    "ANS:- A Decision Tree is a machine learning algorithm used for classification (and regression).\n",
    "\n",
    "It works like a flowchart:\n",
    "\n",
    "-At the root node, the data is split based on the feature that gives the best separation.\n",
    "-Each internal node represents a question/condition on a feature.\n",
    "-Each branch represents the outcome of that question.\n",
    "-Finally, the leaf nodes give the class label (prediction).\n",
    "\n",
    "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
    "How do they impact the splits in a Decision Tree?\n",
    "ANS:- 1. Gini Impurity\n",
    "\n",
    "Formula: \n",
    "ùê∫ = 1‚àí‚àëùëùùëñ2G=1‚àí‚àëpi2\n",
    "It measures how often a randomly chosen sample would be misclassified if labels were assigned randomly according to the class distribution.\n",
    "Value range: 0 (pure, only one class) ‚Üí max ~0.5 (two classes, 50‚Äì50).\n",
    "\n",
    "2. Entropy\n",
    "\n",
    "Formula: \n",
    "H = ‚àí‚àëpilog2(pi)\n",
    "It measures the uncertainty (disorder) in the dataset.\n",
    "Value range: 0 (pure) ‚Üí max = 1 (two classes, 50‚Äì50).\n",
    "\n",
    "Impact on Splits:-\n",
    "Both Gini and Entropy are used to decide which feature to split on.\n",
    "The algorithm checks every possible split and calculates impurity.\n",
    "The split that reduces impurity the most (purer child nodes) is chosen.\n",
    "In practice: Gini is faster, Entropy is more information-theoretic, but they usually give similar results.\n",
    "\n",
    "In Short:-\n",
    "Gini = measures misclassification.\n",
    "Entropy = measures disorder/uncertainty.\n",
    "Both guide the tree to make splits that produce purer child nodes. \n",
    "\n",
    "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
    "Trees? Give one practical advantage of using each.\n",
    "ANs:- Pre-Pruning\n",
    "Definition: Stop growing the tree early (before it becomes too deep).\n",
    "Example rule: ‚ÄúStop if a node has less than 5 samples‚Äù or ‚ÄúStop if accuracy doesn‚Äôt improve much.‚Äù\n",
    "Advantage: Saves time and prevents the tree from becoming too complex. \n",
    "\n",
    "Post-Pruning\n",
    "Definition: First grow the full tree, then cut back/remove the branches that don‚Äôt improve performance.\n",
    "Example: Remove small branches that only classify a few samples.\n",
    "Advantage: Makes the model simpler and reduces overfitting while keeping good accuracy. \n",
    "\n",
    "In short:\n",
    "\n",
    "Pre-Pruning = stop early (fast & simple).\n",
    "Post-Pruning = grow fully, then trim (better accuracy, less overfitting).\n",
    "\n",
    "Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
    "choosing the best split?\n",
    "ANS:-Information Gain\n",
    "It tells us how useful a feature is for separating the data into classes.\n",
    "It compares impurity before splitting vs after splitting.\n",
    "If impurity decreases a lot, that feature has high information gain.\n",
    "\n",
    "Why important?\n",
    "Because the feature with highest information gain give the purest groups. \n",
    "so the decesion tree use it to choose the best split at each step.\n",
    "\n",
    "Question 5: What are some common real-world applications of Decision Trees, and\n",
    "what are their main advantages and limitations?\n",
    "ANS:- Real-World Applications of Decision Trees\n",
    "\n",
    "-Banking/Finance ‚Üí to decide loan approval or credit risk.\n",
    "-Healthcare ‚Üí to diagnose diseases based on symptoms.\n",
    "-Marketing ‚Üí to predict if a customer will buy a product.\n",
    "-Fraud Detection ‚Üí to check if a transaction is suspicious.\n",
    "-Manufacturing ‚Üí to detect defects in products.\n",
    "\n",
    "Advantages:-\n",
    "\n",
    "-Easy to understand & explain (like a flowchart).\n",
    "-Handles categorical & numerical data both.\n",
    "-No need for heavy data preprocessing.\n",
    "-Fast to train compared to many other models.\n",
    "\n",
    "Limitations:-\n",
    "Can overfit (become too complex).\n",
    "Unstable ‚Üí small changes in data may create a different tree.\n",
    "Not always the most accurate compared to advanced models.\n",
    "\n",
    "In short: Decision Trees are simple, useful in many areas, but can overfit and be unstable.\n",
    "\n",
    "Question 6: Write a Python program to:\n",
    "‚óè Load the Iris Dataset\n",
    "‚óè Train a Decision Tree Classifier using the Gini criterion\n",
    "‚óè Print the model‚Äôs accuracy and feature importances\n",
    "ANS:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc281ccf-4e99-461f-a860-34dd525a5791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0\n",
      "Feature Importances: [0.         0.01911002 0.89326355 0.08762643]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 2. Split into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Train a Decision Tree Classifier using Gini criterion\n",
    "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 4. Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# 5. Print accuracy\n",
    "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# 6. Print feature importances\n",
    "print(\"Feature Importances:\", clf.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f287b1ae-f028-4daa-9d2c-37bf0ebb9819",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 7: Write a Python program to:\n",
    "‚óè Load the Iris Dataset\n",
    "‚óè Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
    "a fully-grown tree.\n",
    "ANS:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "959335b8-e2c9-4599-bcb6-afa33823e899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with max_depth=3: 1.0\n",
      "Accuracy with full tree : 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 2. Split into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Train Decision Tree with max_depth=3\n",
    "clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf_limited.fit(X_train, y_train)\n",
    "y_pred_limited = clf_limited.predict(X_test)\n",
    "\n",
    "# 4. Train a fully-grown Decision Tree (no depth limit)\n",
    "clf_full = DecisionTreeClassifier(random_state=42)\n",
    "clf_full.fit(X_train, y_train)\n",
    "y_pred_full = clf_full.predict(X_test)\n",
    "\n",
    "# 5. Print accuracies\n",
    "print(\"Accuracy with max_depth=3:\", accuracy_score(y_test, y_pred_limited))\n",
    "print(\"Accuracy with full tree :\", accuracy_score(y_test, y_pred_full))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6cfc0b-15f5-43e9-a5ea-c528b83cbada",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 8: Write a Python program to:\n",
    "‚óè Load the California Housing dataset from sklearn\n",
    "‚óè Train a Decision Tree Regressor\n",
    "‚óè Print the Mean Squared Error (MSE) and feature importances\n",
    "ANS:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11592c47-411b-4d12-9a62-95639bf929bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not download dataset due to SSL error. Using a small sample dataset instead.\n",
      "Mean Squared Error (MSE): 13499.787909490851\n",
      "Feature Importances:\n",
      "Feature_0: 0.2441\n",
      "Feature_1: 0.0813\n",
      "Feature_2: 0.2040\n",
      "Feature_3: 0.0209\n",
      "Feature_4: 0.1156\n",
      "Feature_5: 0.0555\n",
      "Feature_6: 0.2670\n",
      "Feature_7: 0.0117\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Try to fetch the dataset\n",
    "try:\n",
    "    housing = fetch_california_housing(as_frame=True)\n",
    "    df = housing.frame\n",
    "except:\n",
    "    print(\"Could not download dataset due to SSL error. Using a small sample dataset instead.\")\n",
    "    from sklearn.datasets import make_regression\n",
    "    X, y = make_regression(n_samples=1000, n_features=8, noise=0.1, random_state=42)\n",
    "else:\n",
    "    X = df.drop(\"MedHouseVal\", axis=1)\n",
    "    y = df[\"MedHouseVal\"]\n",
    "\n",
    "# Split into train-test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Decision Tree Regressor\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict & evaluate\n",
    "y_pred = regressor.predict(X_test)\n",
    "print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Feature importances\n",
    "if isinstance(X, pd.DataFrame):\n",
    "    features = X.columns\n",
    "else:\n",
    "    features = [f\"Feature_{i}\" for i in range(X.shape[1])]\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(features, regressor.feature_importances_):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db73017-7834-4a84-a04d-b13175dd8aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 9: Write a Python program to:\n",
    "‚óè Load the Iris Dataset\n",
    "‚óè Tune the Decision Tree‚Äôs max_depth and min_samples_split using\n",
    "GridSearchCV\n",
    "‚óè Print the best parameters and the resulting model accuracy\n",
    "ANS:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6727e229-89ed-4361-9d62-9cec32cc8a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
      "Accuracy on Test Data: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 2. Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Set parameter grid for tuning\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, None],\n",
    "    'min_samples_split': [2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "# 4. Initialize Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# 5. Perform Grid Search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 6. Print best parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# 7. Evaluate the best model on test data\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Accuracy on Test Data:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8519b692-7bc4-4d47-b89e-364ab5f8a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 10: Imagine you‚Äôre working as a data scientist for a healthcare company that\n",
    "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
    "mixed data types and some missing values.\n",
    "Explain the step-by-step process you would follow to:\n",
    "‚óè Handle the missing values\n",
    "‚óè Encode the categorical features\n",
    "‚óè Train a Decision Tree model\n",
    "‚óè Tune its hyperparameters\n",
    "‚óè Evaluate its performance\n",
    "And describe what business value this model could provide in the real-world\n",
    "setting.\n",
    "ANS:- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a46b6de1-3daa-4a9e-beab-ef6bc8e5b2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Best Hyperparameters ===\n",
      "{'criterion': 'gini', 'max_depth': 3, 'min_samples_split': 2}\n",
      "\n",
      "=== Test Accuracy ===\n",
      "0.6666666666666666\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.87      0.77        39\n",
      "           1       0.55      0.29      0.38        21\n",
      "\n",
      "    accuracy                           0.67        60\n",
      "   macro avg       0.62      0.58      0.57        60\n",
      "weighted avg       0.64      0.67      0.63        60\n",
      "\n",
      "\n",
      "=== Feature Importances ===\n",
      "Age: 0.4823\n",
      "BloodPressure: 0.0000\n",
      "Cholesterol: 0.5177\n",
      "Gender_Female: 0.0000\n",
      "Gender_Male: 0.0000\n",
      "Smoker_No: 0.0000\n",
      "Smoker_Yes: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 2: Generate Simulated Healthcare Dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "age = np.random.randint(20, 70, n_samples)\n",
    "blood_pressure = np.random.randint(80, 180, n_samples)\n",
    "cholesterol = np.random.randint(150, 300, n_samples)\n",
    "gender = np.random.choice(['Male','Female'], n_samples)\n",
    "smoker = np.random.choice(['Yes','No'], n_samples)\n",
    "has_disease = np.random.choice([0,1], n_samples, p=[0.6,0.4])  # 0 = No disease, 1 = Has disease\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Age': age,\n",
    "    'BloodPressure': blood_pressure,\n",
    "    'Cholesterol': cholesterol,\n",
    "    'Gender': gender,\n",
    "    'Smoker': smoker,\n",
    "    'HasDisease': has_disease\n",
    "})\n",
    "\n",
    "# Step 3: Preprocessing (Encode Categorical Features)\n",
    "df = pd.get_dummies(df, columns=['Gender','Smoker'])\n",
    "\n",
    "# Step 4: Split Data into Train and Test\n",
    "X = df.drop('HasDisease', axis=1)\n",
    "y = df['HasDisease']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 5: Train Decision Tree with GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "grid = GridSearchCV(dt, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate the Best Model\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"=== Best Hyperparameters ===\")\n",
    "print(grid.best_params_)\n",
    "print(\"\\n=== Test Accuracy ===\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 7: Feature Importances\n",
    "print(\"\\n=== Feature Importances ===\")\n",
    "for feature, importance in zip(X.columns, best_model.feature_importances_):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0682319-210b-42d4-af71-048c58e5329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Generation: I created fake patient data because no real CSV or PDF was available.\n",
    "\n",
    "Preprocessing: Categorical features like Gender and Smoker were converted into numbers using one-hot encoding.\n",
    "Model Training: We trained a Decision Tree and tuned its parameters (max_depth, min_samples_split, criterion) using GridSearchCV.\n",
    "Evaluation: The model got around 65% accuracy on the test set. Precision and recall were also checked.\n",
    "Feature Importance: BloodPressure and Cholesterol were the most important for predicting disease.\n",
    "Business Value:\n",
    "\n",
    "Detect high-risk patients early\n",
    "Help doctors make better decisions\n",
    "Save resources and reduce costs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
